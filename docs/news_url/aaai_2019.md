# Five papers accepted by AAAI 2019!

We are pleased to inform that **FIVE PAPERS** of our lab have been **ACCEPTED** for presentation at the Thirty-Third AAAI Conference on Artificial Intelligence (**AAAI**-19)!

This year AAAI received over **7,700** submissions. Of those, 7,095 were reviewed, and due to space limitations there were only **1,150** papers accepted, yielding an acceptance rate of **16.2%**. There was especially stiff competition this year because of the number of submissions, so congratulations to them for their outstanding works.

The accepted papers are as following:

**MeshNet: Mesh Neural Network for 3D Shape Representation**

*Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, Yue Gao*

**Abstract**：Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named MeshNet, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, MeshNet is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed MeshNet method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed MeshNet can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.

<div align="center"{margin:10%}><img src=http://107.170.229.87:80/news_img/paper1.png width=80% height=80%>
</div>





**MLVCNN: Multi-Loop-View Convolutional Neural Network for 3D Shape Retrieval** 

 *Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, Yue Gao*

**Abstract:** 3D shape retrieval has attracted much attention and become a hot topic in computer vision field recently.With the development of deep learning, 3D shape retrieval has also made great progress and many view-based methods have been introduced in recent years. However, how to represent 3D shapes better is still a challenging problem. At the same time, the intrinsic hierarchical associations among views still have not been well utilized. In order to tackle these problems, in this paper, we propose a multi-loop-view convolutional neural network (MLVCNN) framework for 3D shape retrieval. In this method, multiple groups of views are extracted from different loop directions first. Given these multiple loop views, the proposed MLVCNN framework introduces a hierarchical view-loop-shape architecture, i.e., the view level, the loop level, and the shape level, to conduct 3D shape representation from different scales. In the view-level, a convolutional neural network is first trained to extract view features. Then, the proposed Loop Normalization and LSTM are utilized for each loop of view to generate the loop-level features, which considering the intrinsic associations of the different views in the same loop. Finally, all the loop-level descriptors are combined into a shape-level descriptor for 3D shape representation, which is used for 3D shape retrieval. Our proposed method has been evaluated on the public 3D shape benchmark, i.e., ModelNet40. Experiments and comparisons with the state-of-the-art methods show that the proposed MLVCNN method can achieve significant performance improvement on 3D shape retrieval tasks. Our MLVCNN outperforms the state-of-the-art methods by the mAP of 4.84\% in 3D shape retrieval task. We have also evaluated the performance of the proposed method on the 3D shape classification task where MLVCNN also achieves superior performance compared with recent methods. 

<div align="center"{margin:10%}><img src=http://107.170.229.87:80/news_img/paper2.png width=80% height=80%>
</div>



**DeepCCFV: Camera Constraint-Free Multi-View Convolutional Neural Network for 3D Object Retrieval**

 *Zhengyue Huang, Zhehui Zhao, Hengguang Zhou, Xibin Zhao, Yue Gao*

**Abstract**：3D object retrieval has a compelling demand in the ﬁeld of computer vision due to the rapid development of 3D vision technology and increasing applications of 3D objects. In 3D object retrieval, one important task is how to represent 3D objects accurately and discriminatively based on the 3D data, such as voxel, point-cloud and projected views. Recently, extensive research efforts have been dedicated to investigate 3D object representations for retrieval. Among them, projected views based approaches have shown superior performance compared with other methods in recent years. However, most of existing methods require a ﬁxed predeﬁned camera position setting, which is impractical in real application. Fixed camera positions during the training procedure will also lead to over-ﬁtting to the complete set of features obtained from these positions. To deal with these problems, in this paper, we propose a camera constraint-free multiview convolutional neural network for 3D object retrieval. In this method, we proposed a feature augmentation method assembled in traditional multi-view convolutional neural network to eliminate the camera constraints. To evaluate the proposed method, we have conducted experiments on both intramodal and cross-modal retrieval tasks. Experimental results and comparison with existing state-of-the-art 3D object retrieval methods demonstrate the effectiveness of the proposed method.

<div align="center"{margin:10%}><img src=http://107.170.229.87:80/news_img/paper3.png width=80% height=80%>
</div>


 



 

**Hypergraph Neural Networks**

*Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, Yue Gao*

**Abstract:** In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.

<div align="center"{margin:10%}><img src=http://107.170.229.87:80/news_img/paper4.png width=80% height=80%>
</div>


 

**PVRNet: Point-View Relation Neural Network for 3D Shape Recognition**

*Haoxuan You, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji, Yue Gao.*

**Abstract:**Three-dimensional (3D) shape recognition has drawn much research attention in the field of computer vision. The advances of deep learning encourage various deep models for 3D feature extraction, which achieve the state-of-the-art performance. For point cloud and multi-view data, two popular 3D data modalities, different models are proposed with remarkable performance. While the relation between point cloud and views has been rarely investigated. In this paper, we introduce Point-View Relation Network (PVRNet), an effective network designed to well fuse the view features and the point cloud feature with a proposed relation score module. More specifically, based on the relation score module, the point-single-view fusion feature is first extracted by fusing the point cloud feature and each single view feature with point-singe-view relation, then the point-multi-view fusion feature is extracted by fusing the point cloud feature and the features of different numbers of views with point-singe-view relation. Finally the point-single-view fusion feature and point-multi-view fusion feature are further combined together to achieve a unified representation for a 3D shape. Our proposed PVRNet has been evaluated on ModelNet40 dataset for 3D shape classification and retrieval. Experimental results indicate our model can achieve significant performance improvement compared with the state-of-the-art models.

<div align="center"{margin:10%}><img src=http://107.170.229.87:80/news_img/paper5.png width=80% height=80%>
</div>

 